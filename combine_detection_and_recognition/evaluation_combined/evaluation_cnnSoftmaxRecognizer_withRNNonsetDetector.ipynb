{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJo_DYYBFIRR"
      },
      "source": [
        "SCRIPT_INTRODUCTION = \"\"\"\n",
        "   ==================== [_ESP_Pipe003_For_GitHub_Refactored_EVAL_DetectionAndRecognition.ipynb] ====================\n",
        "  This Script was intended to build a recognition evaluation, for the onsets, using SVM models. Copy this notebooks as a reference\n",
        "  to make it for other models. In this notebook we also \n",
        "  ==================== ==================[ INFO ] ==============================\n",
        "\"\"\"\n",
        "\n",
        "import librosa\n",
        "from google.colab import drive\n",
        "import os,sys,re,pandas as pd,numpy as np\n",
        "import glob\n",
        "import logging\n",
        "from sympy import Interval\n",
        "import warnings"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCaPwqEcFcqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2bd1c58f-1dd7-4818-ab7f-600fcc9c4f6a"
      },
      "source": [
        "ROOT_DIR = \"/content/drive\"\n",
        "drive.mount(ROOT_DIR)\n",
        "MUSIC_DIR = os.path.join(ROOT_DIR,'My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums')\n",
        "model_predictions_path_hardcoded = MUSIC_DIR+'/models/model_predictions_rnn_fulltrained.pkl'\n",
        "MUSIC_DIR"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define NN arquitecture again, for loading the models with .pth instead of pkl or joblib"
      ],
      "metadata": {
        "id": "tmIHifRhbBiI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iD_PM5XHB-i"
      },
      "source": [
        "# TORCH MODULES\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Load the Net module\n",
        "\n",
        "# I unfortunately didn mange hwo to load the models without defyining the class\n",
        "class Net(nn.Module):\n",
        "  \"Generic class for our NN. This must be loaded, if not loaded torch.load our model will throw an errror\"\n",
        "  def __init__(self,nchannels,nclasses, unique_labels, meanstd_normalize):\n",
        "      # start\n",
        "      super().__init__()\n",
        "      # this is the normalizer to used in the predictor then\n",
        "      self.meanstd_normalizer = torchvision.transforms.Normalize(**meanstd_normalize, inplace=False) \n",
        "      # remove it if you want to this is jsut for cleaner predictions (use labels instead of etc)\n",
        "      self.unique_labels = unique_labels\n",
        "      # other attributes\n",
        "      self.nchannels = nchannels\n",
        "      self.nclasses = nclasses\n",
        "      self.conv1 = nn.Conv2d(self.nchannels, 6, 5)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "      self.fc1 = nn.Linear(2000, 120)\n",
        "      self.dropout1 = nn.Dropout(p=0.5, inplace=False)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      #self.dropout2 = nn.Dropout(p=0.3, inplace=False)\n",
        "      self.fc3 = nn.Linear(84, self.nclasses)\n",
        "  def forward(self, x):\n",
        "      \"Prints are commented for debugging purposes\"\n",
        "      # conv1 \n",
        "      x = self.conv1(x)\n",
        "      #print(\"Conv1:\",x.shape)\n",
        "      x =F.relu(x)\n",
        "      x = self.pool(x)\n",
        "      #print(\"Pool1:\",x.shape)\n",
        "      x = self.conv2(x)\n",
        "      #print(\"Conv2:\",x.shape)\n",
        "      x = F.relu(x)\n",
        "      x = self.pool(x)\n",
        "      #print(\"Pool2:\",x.shape)\n",
        "      # flatten  all dims except the batch; \n",
        "      x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      #print(\"Flattened, except batch:\",x.shape)\n",
        "      x = self.fc1(x)\n",
        "      x=F.relu(x)\n",
        "      x = self.dropout1(x)\n",
        "      x = self.fc2(x)\n",
        "      # pass over relu\n",
        "      x = F.relu(x)\n",
        "      #\n",
        "      #x = self.dropout2(x)\n",
        "      # pass over fc3 omg\n",
        "      x = self.fc3(x)\n",
        "      #https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
        "      return x\n",
        "  def map_idx2labels(self,mapped_labels,unique_labels):\n",
        "    labels = list()\n",
        "    for idx in range(len(mapped_labels)):\n",
        "      mapped_lab = mapped_labels[idx]\n",
        "      label_name = unique_labels[mapped_lab]\n",
        "      labels.append(label_name)\n",
        "    return labels\n",
        "\n",
        "  def predict(self,x_batch):\n",
        "    \"\"\"\n",
        "    Final prediction function\n",
        "    params:\n",
        "      x_batch -> np.array model dimensions data_lenx513x17 data\n",
        "    return: mapped prediction (either target label or other)\n",
        "    \"\"\"\n",
        "    #x_batch = x_test[:10].copy()\n",
        "    #N = len(x_batch)\n",
        "    data_tensor = torch.tensor(x_batch, dtype=torch.float32)\n",
        "    N,H,W = data_tensor.shape\n",
        "    #print(N,H,W)\n",
        "    data_tensor = data_tensor.reshape(N,1,H,W)\n",
        "    # noramalize data\n",
        "    data_tensor = self.meanstd_normalizer(data_tensor)\n",
        "    #forward pass\n",
        "    predictions = self.forward(data_tensor)\n",
        "    # calculate the .max(1) for each batch and convert them to list\n",
        "    predictions = predictions.argmax(1).tolist()\n",
        "    predictions2labels = self.map_idx2labels(predictions,self.unique_labels)\n",
        "    return predictions2labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V4bu9odTJ3Q",
        "outputId": "48093575-5580-4a7c-e486-4bba31964dd3"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/tesis_esp\"\n",
        "import eval_utils\n",
        "import pred_utils"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/tesis_esp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You should take models from ./models instead of using this path"
      ],
      "metadata": {
        "id": "kjxycDuKVdEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXP_PIPE_DATA = os.path.join(MUSIC_DIR,'pipe005_multiplemodelsdata_corrected_over60')\n",
        "model_type = \"NN\""
      ],
      "metadata": {
        "id": "8zaZNTbgVq7d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_type == \"SVC\":\n",
        "  # svc takes sparse vector as inputs (flattened) , not the data as a matrix\n",
        "  # therefor needs to be flattened\n",
        "  flatten_data = True\n",
        "  print(\"[INFO ]Loading SVC models\")\n",
        "  # load all models SVC classifier, each of them has it own aparameters ; to acces to them use model_hh.best_estimator_\n",
        "  model_tt = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_TT.joblib'))\n",
        "  model_cy = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_CY.joblib'))\n",
        "  model_kd = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_KD.joblib'))\n",
        "  model_sd = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_SD.joblib'))\n",
        "  model_hh = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_HH.joblib'))\n",
        "  model_ot = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'model_OT.joblib'))\n",
        "elif model_type == \"NN\":\n",
        "  # no need to flatten the data for the conv\n",
        "  flatten_data = False\n",
        "  print(\"[INFO ]Loading NN  models\")\n",
        "  model_tt = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_TT.pth'))\n",
        "  model_cy = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_CY.pth'))\n",
        "  model_kd = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_KD.pth'))\n",
        "  model_sd = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_SD.pth'))\n",
        "  model_hh = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_HH.pth'))\n",
        "  model_ot = pred_utils.load_model(os.path.join(EXP_PIPE_DATA,'nn_model_OT.pth'))\n",
        "else:\n",
        "  raise ValueError(\"Param: model_type must be 'SVC' or 'NN' for now, please choose a valid model\")\n",
        "\n",
        "models_list = [model_tt,model_cy,model_kd,model_sd,model_hh,model_ot]"
      ],
      "metadata": {
        "id": "zugQiY2VVeao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36fc19d3-d613-4af5-8162-2040a9b4658e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO ]Loading NN  models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIR = os.path.join(MUSIC_DIR,'audio','drum_only')\n",
        "ANNOTATIONS_DIR = os.path.join(MUSIC_DIR,'annotations','class')\n",
        "ANNOTATIONS_DIR_TEST = os.path.join(ANNOTATIONS_DIR,'test')\n",
        "annotations_test_filepaths = glob.glob(ANNOTATIONS_DIR_TEST+\"/*.txt\")\n",
        "annotations_test_filepaths"
      ],
      "metadata": {
        "id": "kKruWpC2VyOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073c1b06-c1af-49f5-d11c-e562e971bdf3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Hendrix_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_SwingJazz_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_FreeJazz_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Beatles_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Country1_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_SpeedMetal_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Punk_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_ModalJazz_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Gospel_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_LatinJazz_class.txt',\n",
              " '/content/drive/My Drive/Maestria DM y KDD/Especializacion tesis/MDBDrums/MDB Drums/annotations/class/test/MusicDelta_Grunge_class.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 load the annotations dataset you want to scan\n",
        "# step 2 load the corresponding .wav\n",
        "# step 3 for each sound of the annotations, add the label (break step inot more steps)#\n",
        "# step 4 use evalmetrics class\n",
        "config_signal_params = {\"hop_size\":256,\"n_fft\":1024,\"desired_signal_size_for_padding\":4096,\"seconds_window\":0.05}\n",
        "hop_size, n_fft, desired_signal_size_for_padding, seconds_window  = config_signal_params.values()"
      ],
      "metadata": {
        "id": "IWsJt1i8V5Ro"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1\n",
        "annotation_path = annotations_test_filepaths[0]\n",
        "\n",
        "def pipeline_generate_datasets(annotation_path: str, onsets_annotation_list = None):\n",
        "  \"\"\"\n",
        "  --------------------------------------------------------------------------------\n",
        "  Given an annotation path, this function will geneate datasets with \n",
        "  predictions on a .wav and its annotation (loading the .txt);\n",
        "   this is a pre function for calculatiing precision and recall\n",
        "   --------------------------------------------------------------------------------\n",
        "  args: \n",
        "    annotation_path -> .wav song ; a .txt with the same name must exist\n",
        "    onsets_annotation_list -> annnotations that could come from an onset detector \n",
        "                          or from annotations by an user. \n",
        "                          it is a list with the times an onset occurs like\n",
        "                          [0.15,0.32,..] and that means an onset occured\n",
        "                          at the second .15 and .32 (that could be detected by\n",
        "                          anonset detector or just annotations).\n",
        "                          If onsets_annotation_list we will use the annotations\n",
        "                          from the MDB Drums\n",
        "\n",
        "                          \n",
        "  return:\n",
        "    df_annotation -> annotations loaded into dataframe\n",
        "    df_predicted -> predictiosn from the model (comes from pred_utils models)\n",
        "  In case you wanna change the models you ened to change : pred_utils.config_signal_params, sklearn_models_list within the function; the rest will be the same\n",
        "  \"\"\"\n",
        "  df_annotation = eval_utils.load_labels(annotation_path,set2df = True)\n",
        "  # step 2: load correponsding .wav\n",
        "  # 2.a search path\n",
        "  wav_path = eval_utils.search_correspondingpath_given_annotation(annotation_path_txt = annotation_path, audio_directory = AUDIO_DIR)\n",
        "  # 2.b now load the wav\n",
        "  signal,sampling_rate = librosa.load(wav_path)\n",
        "  # 3.c) now use annotations onset, here you could use an onset detection method aswell  and DruMTypesDetector's instance to create a dataset with annotations and predictions \n",
        "  # 3.c.i) first set onset time to onset_sample\n",
        "  df_annotation[\"onset_sample\"] = (df_annotation[\"onset_time\"].astype(\"float\")*sampling_rate).astype(\"int\")\n",
        "\n",
        "\n",
        "  if onsets_annotation_list is None:\n",
        "    print(\"[INFO] Since onsets_annotation_list is None, we will be using labeled onsets for presettled onsets; if you have an OnsetDetector, put their predicts output in this param \")\n",
        "    onsets_annotation_list = df_annotation[\"onset_sample\"].tolist()\n",
        "  else:\n",
        "    print(\"[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\")\n",
        "  # 3.d) now perform the predicts and put this into a dataframe\n",
        "  drum_types_detector = pred_utils.DrumTypesDetector(config_signal_params,\n",
        "                                        sklearn_models_list = models_list,\n",
        "                                        flatten_data = flatten_data)\n",
        "  drum_types_detector(wav_path, presettled_onsets= onsets_annotation_list)\n",
        "  df_predicted = pd.DataFrame(drum_types_detector.list_formatted_onsets, columns=[\"onset_time\",\"predicted_drumtype\"])\n",
        "  # now write annotation path for both datasets\n",
        "  df_predicted[\"annotation_path\"] = annotation_path\n",
        "  df_annotation[\"annotation_path\"] = annotation_path\n",
        "  return df_annotation, df_predicted\n",
        "\n"
      ],
      "metadata": {
        "id": "168xqu8vV80u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First load rnn osnet detector predicitons taken from the other script (onset detector rnn)"
      ],
      "metadata": {
        "id": "Xquo8ndTy8zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for all songs\n",
        "sampling_rate =  int(44100/2)\n",
        "df_predictions_rnn_onsetdetector = pd.read_pickle(model_predictions_path_hardcoded)\n",
        "# generate sample columns\n",
        "df_predictions_rnn_onsetdetector[\"onset_sample\"] =  (df_predictions_rnn_onsetdetector[\"onset_time\"].astype(\"float\")*sampling_rate).astype(\"int\")\n",
        "# replace audio path in order to match annotations dataset ; anotehr smarter way would be to just pick up names instead of fullpath --> we will do it later\n",
        "df_predictions_rnn_onsetdetector[\"audio_path\"] = df_predictions_rnn_onsetdetector[\"audio_path\"].str.replace(\"_Drum.wav\",\"_class.txt\").str.replace(\"/audio/drum_only/\",\"/annotations/class/\")\n",
        "df_predictions_rnn_onsetdetector.head()"
      ],
      "metadata": {
        "id": "_id3EH4My532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "9a6e35cc-e543-4d2d-d114-f435de2bfce4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   onset_time                                         audio_path  onset_sample\n",
              "0        0.02  /content/drive/My Drive/Maestria DM y KDD/Espe...           441\n",
              "1        0.29  /content/drive/My Drive/Maestria DM y KDD/Espe...          6394\n",
              "2        0.57  /content/drive/My Drive/Maestria DM y KDD/Espe...         12568\n",
              "3        0.85  /content/drive/My Drive/Maestria DM y KDD/Espe...         18742\n",
              "4        1.00  /content/drive/My Drive/Maestria DM y KDD/Espe...         22050"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f1ac24e-255e-44bb-a4e7-1195640a1764\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>onset_time</th>\n",
              "      <th>audio_path</th>\n",
              "      <th>onset_sample</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.02</td>\n",
              "      <td>/content/drive/My Drive/Maestria DM y KDD/Espe...</td>\n",
              "      <td>441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.29</td>\n",
              "      <td>/content/drive/My Drive/Maestria DM y KDD/Espe...</td>\n",
              "      <td>6394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.57</td>\n",
              "      <td>/content/drive/My Drive/Maestria DM y KDD/Espe...</td>\n",
              "      <td>12568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.85</td>\n",
              "      <td>/content/drive/My Drive/Maestria DM y KDD/Espe...</td>\n",
              "      <td>18742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.00</td>\n",
              "      <td>/content/drive/My Drive/Maestria DM y KDD/Espe...</td>\n",
              "      <td>22050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f1ac24e-255e-44bb-a4e7-1195640a1764')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f1ac24e-255e-44bb-a4e7-1195640a1764 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f1ac24e-255e-44bb-a4e7-1195640a1764');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now generate the predictions of drum types for all onset detected in df_predictions_rnn_onsetdetector"
      ],
      "metadata": {
        "id": "T_LdCBCSZNUJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIqnAgmhhApE",
        "outputId": "46eec65b-3dc0-4438-8983-cb4a11d06075"
      },
      "source": [
        "# build a huge dataset with this pipeline\n",
        "annotation_list, predicted_list = list(), list()\n",
        "#  vbuild a dataset with all paths\n",
        "\n",
        "counter = 0\n",
        "total_files2process = len(annotations_test_filepaths)\n",
        "for annotation_path in annotations_test_filepaths:\n",
        "  print(\"Proportion of processed:\",round(counter/total_files2process,2))\n",
        "\n",
        "  # load the predetected onset samples by our model\n",
        "  predetected_onset_samples = df_predictions_rnn_onsetdetector[ df_predictions_rnn_onsetdetector[\"audio_path\"] == annotation_path ][\"onset_sample\"].tolist()\n",
        "  # now  input the predetected onset by the rnn\n",
        "  df_annotation, df_predicted = pipeline_generate_datasets(annotation_path, onsets_annotation_list = predetected_onset_samples)\n",
        "  annotation_list.append(df_annotation)\n",
        "  predicted_list.append(df_predicted)\n",
        "  counter += 1\n",
        "# now create a df based on the list of dfs\n",
        "df_annotation_all = pd.concat(annotation_list)\n",
        "df_predicted_all = pd.concat(predicted_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of processed: 0.0\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.09\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.18\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.27\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.36\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.45\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.55\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.64\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.73\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.82\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n",
            "Proportion of processed: 0.91\n",
            "[INFO] Since onsets_annotation_list is NOT None, this is a Detection+Recognition task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute metrics (F1,recall, precision) for each drumtype"
      ],
      "metadata": {
        "id": "P8i-Hs3ymB_c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcjjxX2thDUj",
        "outputId": "1ed781c6-fdcc-4be2-e9b1-cec08679148e"
      },
      "source": [
        "# criterion; this criterion is for onset detection;\n",
        "# but take into account that you are passing five models, \n",
        "# for example if the HH is on second 0.02 and SD on 0.03 ; using window time\n",
        "# you can recognize the HH at second 0.03 and SD at second 0.02; and this will be correct (since it is part of the same STFT / sound)\n",
        "# therefore no need to change sample criterion so much (0.01 is min in recognition)\n",
        "seconds_criterion = 0.03\n",
        "# this is for all of our songs (see it in the librosa.laod return)\n",
        "sampling_rate =  int(44100/2)\n",
        "# add onset_sample which if forgot\n",
        "df_predicted_all[\"onset_sample\"] = (df_predicted_all[\"onset_time\"]*sampling_rate).astype(\"int\")\n",
        "df_predicted_all[\"drum_type\"] = df_predicted_all[\"predicted_drumtype\"]\n",
        "#\n",
        "samples_criterion = int(seconds_criterion*sampling_rate)\n",
        "# all drumtypes\n",
        "drum_types_list = [\"KD\",\"SD\",\"HH\",\"CY\",\"OT\",\"TT\"]\n",
        "\n",
        "# list all the metrics for each drumtype\n",
        "metrics_dict_list = list()\n",
        "for drum_type in drum_types_list:\n",
        "  compute_metrics = eval_utils.ComputeMetrics(true_labels = df_annotation_all, predicted_labels = df_predicted_all, samples_criterion = samples_criterion, filter_drumtype = drum_type)\n",
        "  metrics_dict = compute_metrics()\n",
        "  metrics_dict[\"drum_type\"] = drum_type\n",
        "  metrics_dict_list.append(metrics_dict)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/tesis_esp/eval_utils.py:106: UserWarning: [WARNING] n_detected_onsets_within_range > 1!\n",
            "  warnings.warn(\"[WARNING] n_detected_onsets_within_range > 1!\")\n",
            "/content/drive/My Drive/Colab Notebooks/tesis_esp/eval_utils.py:147: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  self.precision = np.sum(total_matches_precision)/self.precision_denominator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show metrics "
      ],
      "metadata": {
        "id": "ZfNWWKHtmF7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Pwn7kbSxl3",
        "outputId": "54436beb-21e2-4040-cdbf-d66d7640b761"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'recall': 0.9635535307517085,\n",
              "  'precision': 0.9724137931034482,\n",
              "  'f1_score': 0.9679633867276888,\n",
              "  'drum_type': 'KD'},\n",
              " {'recall': 0.9022222222222223,\n",
              "  'precision': 0.9115442278860569,\n",
              "  'f1_score': 0.9068592694369233,\n",
              "  'drum_type': 'SD'},\n",
              " {'recall': 0.7268339768339769,\n",
              "  'precision': 0.7917981072555205,\n",
              "  'f1_score': 0.7579265223955711,\n",
              "  'drum_type': 'HH'},\n",
              " {'recall': 0.4421768707482993,\n",
              "  'precision': 0.7168141592920354,\n",
              "  'f1_score': 0.5469561603989196,\n",
              "  'drum_type': 'CY'},\n",
              " {'recall': 0.0, 'precision': nan, 'f1_score': nan, 'drum_type': 'OT'},\n",
              " {'recall': 0.0, 'precision': nan, 'f1_score': nan, 'drum_type': 'TT'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}